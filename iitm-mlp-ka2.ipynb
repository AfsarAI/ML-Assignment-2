{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":107399,"databundleVersionId":13009703,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-23T16:29:35.251562Z","iopub.execute_input":"2025-07-23T16:29:35.251848Z","iopub.status.idle":"2025-07-23T16:29:35.591060Z","shell.execute_reply.started":"2025-07-23T16:29:35.251826Z","shell.execute_reply":"2025-07-23T16:29:35.590241Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Loading the data\n","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/mlp-term-2-2025-kaggle-assignment-2/train.csv')\ntest_df = pd.read_csv('/kaggle/input/mlp-term-2-2025-kaggle-assignment-2/test.csv')\n\ntrain_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T16:29:35.592816Z","iopub.execute_input":"2025-07-23T16:29:35.593208Z","iopub.status.idle":"2025-07-23T16:29:35.959821Z","shell.execute_reply.started":"2025-07-23T16:29:35.593184Z","shell.execute_reply":"2025-07-23T16:29:35.958983Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Criteria 1\n## Identify data types of different columns","metadata":{}},{"cell_type":"code","source":"train_df.dtypes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T16:29:35.960658Z","iopub.execute_input":"2025-07-23T16:29:35.960949Z","iopub.status.idle":"2025-07-23T16:29:35.967790Z","shell.execute_reply.started":"2025-07-23T16:29:35.960927Z","shell.execute_reply":"2025-07-23T16:29:35.967078Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Criteria 2\n## Present descriptive statistics of numerical columns","metadata":{}},{"cell_type":"code","source":"train_df.describe().T","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T16:29:35.968640Z","iopub.execute_input":"2025-07-23T16:29:35.969105Z","iopub.status.idle":"2025-07-23T16:29:36.075265Z","shell.execute_reply.started":"2025-07-23T16:29:35.969072Z","shell.execute_reply":"2025-07-23T16:29:36.074471Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Criteria 3\n## Identify and handle the missing values","metadata":{}},{"cell_type":"code","source":"train_df.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T16:29:36.078163Z","iopub.execute_input":"2025-07-23T16:29:36.078570Z","iopub.status.idle":"2025-07-23T16:29:36.116428Z","shell.execute_reply.started":"2025-07-23T16:29:36.078538Z","shell.execute_reply":"2025-07-23T16:29:36.115559Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T16:29:36.117421Z","iopub.execute_input":"2025-07-23T16:29:36.117732Z","iopub.status.idle":"2025-07-23T16:29:36.132426Z","shell.execute_reply.started":"2025-07-23T16:29:36.117703Z","shell.execute_reply":"2025-07-23T16:29:36.131550Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# We will impute missing values\n* credit_score:- 9556 & 3185 null values in train and test df respectively, will impute using median as it is more robust to outliers.\n* country:- 6021 & 4606 null values in train and test df respectively, will impute using mode because it is a categorical feature.\n* acc_balance:- 7257 & 5251 null values in train and test df respectively, will impute using median.\n* prod_count:- 4863 & 1717 null values in train and test df respectively, will impute using median","metadata":{}},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nmean_col=['credit_score', 'acc_balance','prod_count']\nmode_col=['country']\n\nmean_imputer=SimpleImputer(strategy='mean')\nmode_imputer=SimpleImputer(strategy='most_frequent')\n\ntrain_df[mean_col]=mean_imputer.fit_transform(train_df[mean_col])\ntrain_df[mode_col]=mode_imputer.fit_transform(train_df[mode_col])\n\ntest_df[mean_col]= mean_imputer.transform(test_df[mean_col])\ntest_df[mode_col]= mode_imputer.transform(test_df[mode_col])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T16:29:36.133458Z","iopub.execute_input":"2025-07-23T16:29:36.133787Z","iopub.status.idle":"2025-07-23T16:29:37.180227Z","shell.execute_reply.started":"2025-07-23T16:29:36.133760Z","shell.execute_reply":"2025-07-23T16:29:37.179403Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T16:29:37.181180Z","iopub.execute_input":"2025-07-23T16:29:37.181581Z","iopub.status.idle":"2025-07-23T16:29:37.210156Z","shell.execute_reply.started":"2025-07-23T16:29:37.181555Z","shell.execute_reply":"2025-07-23T16:29:37.209417Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Criteria 4\n## Identify and handle duplicates","metadata":{}},{"cell_type":"code","source":"train_df.drop(\"id\", axis=1, inplace=True)\ntrain_df = train_df[train_df.duplicated()==False]\ntrain_df.reset_index(inplace=True, drop=True)\nprint(\"Duplicates:\", train_df.duplicated().sum())\n#No duplicates","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T16:29:37.211174Z","iopub.execute_input":"2025-07-23T16:29:37.212039Z","iopub.status.idle":"2025-07-23T16:29:37.345547Z","shell.execute_reply.started":"2025-07-23T16:29:37.212015Z","shell.execute_reply":"2025-07-23T16:29:37.344623Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Criteria 5\n## Identify and handle outliers","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nnumerical_cols = ['credit_score','age','tenure','estimated_salary','acc_balance']\n\nplt.figure(figsize=(15, 20))\nfor i, col in enumerate(numerical_cols, 1):\n    plt.subplot(len(numerical_cols), 1, i)\n    sns.boxplot(x=train_df[col])\n    plt.title(f'Boxplot for {col}')\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T16:29:37.346543Z","iopub.execute_input":"2025-07-23T16:29:37.346869Z","iopub.status.idle":"2025-07-23T16:29:38.649152Z","shell.execute_reply.started":"2025-07-23T16:29:37.346840Z","shell.execute_reply":"2025-07-23T16:29:38.648128Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Keeping the outliers\n* credit_score\n    \n      1. Lower outliers below ~400.\n      2. A low credit score is not an error — it's a valid and important indicator of financial risk.\n      3. Customers with poor credit might be more likely to exit or churn — this pattern could help the model.\n* age\n\n      1. Outliers above ~60–90 years.\n      2. Senior customers may behave differently in retention\n      3. Age extremes are informative, not noise. ","metadata":{}},{"cell_type":"markdown","source":"# Criteria 6\n## Present at least three visualizations and provide insights for the same","metadata":{}},{"cell_type":"code","source":"# Exit rate by country\nsns.countplot(x='country', hue='exit_status', data=train_df)\nplt.title('Churn Rate by Country')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T16:29:38.650279Z","iopub.execute_input":"2025-07-23T16:29:38.650712Z","iopub.status.idle":"2025-07-23T16:29:38.873565Z","shell.execute_reply.started":"2025-07-23T16:29:38.650687Z","shell.execute_reply":"2025-07-23T16:29:38.872722Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Observation\n* France has the largest number of customers, but a lower proportion of churners compared to its total customer base.\n\n* Germany has a relatively high churn rate, as the number of churned customers (exit_status = 1) is closer in proportion to those who stayed.\n\n* Spain has moderate churn counts but lower than France and Germany overall.","metadata":{}},{"cell_type":"code","source":"# Age distribution for churned customers\nsns.histplot(train_df[train_df['exit_status'] == 1]['age'], bins=30, kde=True)\nplt.title(\"Age Distribution of Exiting Customers\")\nplt.xlabel(\"Age\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T16:29:38.874462Z","iopub.execute_input":"2025-07-23T16:29:38.874754Z","iopub.status.idle":"2025-07-23T16:29:39.310226Z","shell.execute_reply.started":"2025-07-23T16:29:38.874727Z","shell.execute_reply":"2025-07-23T16:29:39.309380Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Observation\n* The churn rate peaks around age 45 — this age group has the highest number of exits.\n\n* The distribution is right-skewed, indicating fewer churners among older customers (age 60+).\n\n* Very young customers (under 30) also churn less frequently than middle-aged ones.\n\n* Middle-aged customers (35–50 years old) are the most likely to churn, based on this distribution.\nThis group likely has more financial products, responsibilities, or service expectations making them more sensitive to dissatisfaction.","metadata":{}},{"cell_type":"code","source":"#Credit Score by Exit Status\nplt.figure(figsize=(10, 5))\nsns.histplot(data=train_df, x='credit_score', hue='exit_status', bins=30, kde=True, palette='Set1', alpha=0.6)\nplt.title(\"Credit Score Distribution: Exited vs Stayed\")\nplt.xlabel(\"Credit Score\")\nplt.ylabel(\"Customer Count\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T16:29:39.311209Z","iopub.execute_input":"2025-07-23T16:29:39.311484Z","iopub.status.idle":"2025-07-23T16:29:40.092697Z","shell.execute_reply.started":"2025-07-23T16:29:39.311463Z","shell.execute_reply":"2025-07-23T16:29:40.091665Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Observation\n1. Stayed customers (exit_status = 0): \n    * Have a wide distribution centered around scores of 650–700.\n    * Count drops off after 750 and below 600.\n2. Exited customers (exit_status = 1):\n    * Also concentrated in the 600–700 range, but with fewer customers across all score   bands.\n    * Interestingly, many churned customers also have decent credit scores, meaning good credit alone doesn't guarantee retention.","metadata":{}},{"cell_type":"markdown","source":"# Criteria 7\n## Scale Numerical features and Encode Categorical features","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\nnum_cols = ['credit_score', 'age', 'tenure', 'acc_balance', 'prod_count', 'estimated_salary']\ncat_cols = ['country', 'gender', 'has_card', 'is_active']\n\ntree_preprocessor = ColumnTransformer([\n    ('num', 'passthrough', num_cols),\n    ('cat', OneHotEncoder(handle_unknown='ignore', drop='if_binary'), cat_cols)\n])\n\nmlp_preprocessor = ColumnTransformer([\n    ('num', StandardScaler(), num_cols),\n    ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T16:29:40.096818Z","iopub.execute_input":"2025-07-23T16:29:40.097704Z","iopub.status.idle":"2025-07-23T16:29:40.104309Z","shell.execute_reply.started":"2025-07-23T16:29:40.097670Z","shell.execute_reply":"2025-07-23T16:29:40.103391Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Reasoning for scaling numerical features\nNumerical features like credit_score, age, tenure, etc., vary in scale — some are in hundreds (like credit_score), while others are in single digits (like tenure or prod_count). This variation can negatively impact models that:\n* Are sensitive to feature scale (e.g., Logistic Regression, SVM, KNN, SGD, Neural Networks).\n\nScaling ensures that all features contribute equally to the learning process and prevents dominance of high-magnitude features.\n\n## Reason for choosing StandardScaler\nStandardScaler standardizes features by removing the mean and scaling to unit variance (i.e., it transforms the distribution to have mean = 0 and standard deviation = 1).\nWe chose StandardScaler because it works well with most models that assume or benefit from normalized inputs.\n\n## Reason for encoding categorical features\nMachine learning models can’t interpret textual or categorical variables directly. These need to be converted into numeric values. If left unencoded, models like logistic regression or tree ensembles can’t use them effectively.\n\n## Reason for choosing OneHotEncoder\nOneHotEncoder creates a new binary column for each category. This is the safest and most general encoding method for nominal (unordered) categorical variables. It works best for non-ordinal categorical variables, and it avoids misleading numerical ordering.","metadata":{}},{"cell_type":"markdown","source":"# Criteria 8\n## Model Building (at least 7)","metadata":{}},{"cell_type":"code","source":"!pip install xgboost","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T16:29:40.105257Z","iopub.execute_input":"2025-07-23T16:29:40.105580Z","iopub.status.idle":"2025-07-23T16:29:44.878492Z","shell.execute_reply.started":"2025-07-23T16:29:40.105550Z","shell.execute_reply":"2025-07-23T16:29:44.877619Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import f1_score, roc_auc_score\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import VotingClassifier\n\n\nX = train_df.drop('exit_status', axis=1)\ny = train_df['exit_status']\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n\nmodels = {\n    \"Logistic Regression\": LogisticRegression(max_iter=500, class_weight='balanced',random_state=42),\n    \"Decision Tree\": DecisionTreeClassifier(class_weight='balanced',random_state=42),\n    \"Random Forest\": RandomForestClassifier(class_weight='balanced',random_state=42),\n    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),  \n    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss', \n                              scale_pos_weight=(y == 0).sum() / (y == 1).sum(),random_state=42),\n    \"KNN\": KNeighborsClassifier(),  \n    \"Neural Net\": MLPClassifier(max_iter=300, early_stopping=True), \n    \"AdaBoost\": AdaBoostClassifier(n_estimators=100,random_state=42),\n    \"SGDClassifier\": SGDClassifier(loss='log_loss', max_iter=1000, random_state=42, class_weight='balanced')\n}\n\nscaled_models = [\n    \"Logistic Regression\", \"KNN\", \"Neural Net\", \"SGDClassifier\"\n]\n\nprint(\"Model Performance on Validation Set:\")\nprint(\"-\" * 60)\n\nfor name, model in models.items():\n    preprocessor = mlp_preprocessor if name in scaled_models else tree_preprocessor\n\n    pipeline = Pipeline([\n        ('preprocessor', preprocessor),\n        ('classifier', model)\n    ])\n    \n    pipeline.fit(X_train, y_train)\n    y_pred = pipeline.predict(X_val)\n    y_prob = pipeline.predict_proba(X_val)[:, 1] if hasattr(pipeline.named_steps['classifier'], 'predict_proba') else None\n\n    f1 = f1_score(y_val, y_pred)\n    roc_auc = roc_auc_score(y_val, y_prob) if y_prob is not None else \"N/A\"\n    \n    print(f\"{name:<20} | F1 Score: {f1:.4f} | ROC AUC: {roc_auc if roc_auc=='N/A' else round(roc_auc, 4)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T16:29:44.879641Z","iopub.execute_input":"2025-07-23T16:29:44.879947Z","iopub.status.idle":"2025-07-23T16:30:33.345223Z","shell.execute_reply.started":"2025-07-23T16:29:44.879919Z","shell.execute_reply":"2025-07-23T16:30:33.344290Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_cols = ['credit_score', 'age', 'tenure', 'acc_balance', 'prod_count', 'estimated_salary']\ncat_cols = ['country', 'gender', 'has_card', 'is_active']\n\ntree_preprocessor = ColumnTransformer([\n    ('num', 'passthrough', num_cols),\n    ('cat', OneHotEncoder(handle_unknown='ignore', drop='if_binary'), cat_cols)\n])\n\nmlp_preprocessor = ColumnTransformer([\n    ('num', StandardScaler(), num_cols),\n    ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)\n])\n\nxgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', \n                    scale_pos_weight=(y == 0).sum() / (y == 1).sum(), random_state=42)\ngb = GradientBoostingClassifier(random_state=42)\nmlp = MLPClassifier(max_iter=300, early_stopping=True, random_state=42)\n\nxgb_pipe = Pipeline([\n    ('preprocessor', tree_preprocessor),\n    ('classifier', xgb)\n])\n\ngb_pipe = Pipeline([\n    ('preprocessor', tree_preprocessor),\n    ('classifier', gb)\n])\n\nmlp_pipe = Pipeline([\n    ('preprocessor', mlp_preprocessor),\n    ('classifier', mlp)\n])\n\n\nvoting_clf = VotingClassifier(\n    estimators=[\n        ('xgb', xgb_pipe),\n        ('gb', gb_pipe),\n        ('mlp', mlp_pipe),\n    ],\n    voting='soft'\n)\n\nvoting_clf.fit(X_train, y_train)\ny_pred_vote = voting_clf.predict(X_val)\ny_prob_vote = voting_clf.predict_proba(X_val)[:, 1]\n\nf1_vote = f1_score(y_val, y_pred_vote)\nroc_auc_vote = roc_auc_score(y_val, y_prob_vote)\n\nprint(f\"VotingClassifier        | F1 Score: {f1_vote:.4f} | ROC AUC: {roc_auc_vote:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T16:30:33.346214Z","iopub.execute_input":"2025-07-23T16:30:33.347033Z","iopub.status.idle":"2025-07-23T16:30:52.643392Z","shell.execute_reply.started":"2025-07-23T16:30:33.347006Z","shell.execute_reply":"2025-07-23T16:30:52.642528Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Criteria 9\n## Hyperparameter Tuning on any 3 of the models","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV, StratifiedKFold, RandomizedSearchCV\nfrom scipy.stats import randint, uniform\nfrom sklearn.pipeline import Pipeline\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import BaggingClassifier\n\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T16:30:52.644303Z","iopub.execute_input":"2025-07-23T16:30:52.644571Z","iopub.status.idle":"2025-07-23T16:30:52.649867Z","shell.execute_reply.started":"2025-07-23T16:30:52.644550Z","shell.execute_reply":"2025-07-23T16:30:52.649037Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mlp_pipe = Pipeline([\n    ('preprocessor', mlp_preprocessor),\n    ('classifier', MLPClassifier(max_iter=1000, early_stopping=True, random_state=42))\n])\n\nmlp_param_dist = {\n    'classifier__hidden_layer_sizes': [(100,), (100, 50), (150, 100), (128, 64)],\n    'classifier__activation': ['relu', 'tanh'],\n    'classifier__alpha': uniform(0.0001, 0.01),\n    'classifier__learning_rate_init': uniform(0.001, 0.009),\n    'classifier__solver': ['adam', 'sgd'],\n    'classifier__batch_size': [64, 128, 'auto']\n}\n\nmlp_search = RandomizedSearchCV(\n    mlp_pipe,\n    mlp_param_dist,\n    scoring='f1',\n    cv=cv,\n    n_iter=30,\n    verbose=1,\n    n_jobs=-1,\n    random_state=42\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T16:30:52.650719Z","iopub.execute_input":"2025-07-23T16:30:52.651030Z","iopub.status.idle":"2025-07-23T16:30:52.671818Z","shell.execute_reply.started":"2025-07-23T16:30:52.651004Z","shell.execute_reply":"2025-07-23T16:30:52.670872Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"xgb_pipe = Pipeline([\n    ('preprocessor', tree_preprocessor),\n    ('classifier', XGBClassifier(use_label_encoder=False, eval_metric='logloss',\n                                 scale_pos_weight=(y == 0).sum() / (y == 1).sum(),\n                                 random_state=42))\n])\n\nxgb_param_dist = {\n    'classifier__n_estimators': randint(100, 300),\n    'classifier__learning_rate': uniform(0.01, 0.2),\n    'classifier__max_depth': randint(3, 10),\n    'classifier__subsample': uniform(0.7, 0.3),\n    'classifier__colsample_bytree': uniform(0.6, 0.4),\n    'classifier__gamma': uniform(0, 0.3),\n    'classifier__reg_alpha': uniform(0, 0.2),\n    'classifier__reg_lambda': uniform(0.5, 1.5)\n}\n\nxgb_search = RandomizedSearchCV(\n    xgb_pipe,\n    xgb_param_dist,\n    scoring='f1',\n    cv=cv,\n    n_iter=30,\n    verbose=1,\n    n_jobs=-1,\n    random_state=42\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T16:30:52.672916Z","iopub.execute_input":"2025-07-23T16:30:52.673284Z","iopub.status.idle":"2025-07-23T16:30:52.702424Z","shell.execute_reply.started":"2025-07-23T16:30:52.673257Z","shell.execute_reply":"2025-07-23T16:30:52.701384Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gb_pipe = Pipeline([\n    ('preprocessor', tree_preprocessor),\n    ('classifier', GradientBoostingClassifier(random_state=42))\n])\n\ngb_param_dist = {\n    'classifier__n_estimators': randint(100, 250),\n    'classifier__learning_rate': uniform(0.01, 0.2),\n    'classifier__max_depth': randint(3, 10),\n    'classifier__subsample': uniform(0.7, 0.3),\n    'classifier__min_samples_split': randint(2, 10),\n    'classifier__min_samples_leaf': randint(1, 5),\n    'classifier__max_features': ['sqrt', 'log2', None]\n}\n\ngb_search = RandomizedSearchCV(\n    gb_pipe,\n    gb_param_dist,\n    scoring='f1',\n    cv=cv,\n    n_iter=30,\n    verbose=1,\n    n_jobs=-1,\n    random_state=42\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T16:30:52.703806Z","iopub.execute_input":"2025-07-23T16:30:52.704129Z","iopub.status.idle":"2025-07-23T16:30:52.715421Z","shell.execute_reply.started":"2025-07-23T16:30:52.704106Z","shell.execute_reply":"2025-07-23T16:30:52.714373Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"xgb_search.fit(X_train, y_train)\nprint(\"\\nBest XGB F1 Score:\", xgb_search.best_score_)\nprint(\"Best XGB Params:\", xgb_search.best_params_)\n\ngb_search.fit(X_train, y_train)\nprint(\"\\nBest Gradient Boost F1 Score:\", gb_search.best_score_)\nprint(\"Best GB Params:\", gb_search.best_params_)\n\nmlp_search.fit(X_train, y_train)\nprint(\"\\nBest MLP F1 Score:\", mlp_search.best_score_)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T16:30:52.716310Z","iopub.execute_input":"2025-07-23T16:30:52.716608Z","iopub.status.idle":"2025-07-23T17:00:01.794716Z","shell.execute_reply.started":"2025-07-23T16:30:52.716580Z","shell.execute_reply":"2025-07-23T17:00:01.793858Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Checking tuned models on Validation set","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import f1_score, roc_auc_score\n\nxgb_tuned = xgb_search.best_estimator_\ngb_tuned = gb_search.best_estimator_\nmlp_tuned = mlp_search.best_estimator_\n\n\nfor name, model in zip(['XGBoost', 'Gradient Boosting', 'MLP'], [xgb_tuned, gb_tuned, mlp_tuned]):\n    y_pred = model.predict(X_val)\n    y_prob = model.predict_proba(X_val)[:, 1]\n\n    f1 = f1_score(y_val, y_pred)\n    roc_auc = roc_auc_score(y_val, y_prob)\n\n    print(f\"{name} (Tuned) - F1 Score: {f1:.4f}, ROC AUC: {roc_auc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T17:00:01.795688Z","iopub.execute_input":"2025-07-23T17:00:01.795972Z","iopub.status.idle":"2025-07-23T17:00:02.174892Z","shell.execute_reply.started":"2025-07-23T17:00:01.795944Z","shell.execute_reply":"2025-07-23T17:00:02.173942Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Trying voting classifier on tuned models","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import f1_score, roc_auc_score\n\nxgbVC_pipe = Pipeline([\n    ('preprocessor', tree_preprocessor),\n    ('classifier', xgb_search.best_estimator_.named_steps['classifier'])\n])\n\ngbVC_pipe = Pipeline([\n    ('preprocessor', tree_preprocessor),\n    ('classifier', gb_search.best_estimator_.named_steps['classifier'])\n])\n\nmlpVC_pipe = Pipeline([\n    ('preprocessor', mlp_preprocessor),\n    ('classifier', mlp_search.best_estimator_.named_steps['classifier'])\n])\n\n\nvoting_clf_tuned = VotingClassifier(\n    estimators=[\n        ('xgb', xgbVC_pipe),\n        ('gb', gbVC_pipe),\n        ('mlp', mlpVC_pipe),\n    ],\n    voting='soft',\n    weights=[6, 5, 4]\n)\n\nvoting_clf_tuned.fit(X_train, y_train)\ny_prob_vote = voting_clf_tuned.predict_proba(X_val)[:, 1]\ny_pred_vote = (y_prob_vote >= 0.40).astype(int)\n\nf1_vote = f1_score(y_val, y_pred_vote)\nroc_auc_vote = roc_auc_score(y_val, y_prob_vote)\n\nprint(\"VotingClassifier (Tuned Models + Separate Preprocessing)\")\nprint(f\"F1 Score : {f1_vote:.4f}\")\nprint(f\"ROC AUC  : {roc_auc_vote:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T17:00:02.175906Z","iopub.execute_input":"2025-07-23T17:00:02.176253Z","iopub.status.idle":"2025-07-23T17:00:43.393986Z","shell.execute_reply.started":"2025-07-23T17:00:02.176225Z","shell.execute_reply":"2025-07-23T17:00:43.393067Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"VotingClassifier (Tuned Models + Separate Preprocessing)\nF1 Score : 0.6425\nROC AUC  : 0.8791","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import f1_score\n\nvoting_probs = voting_clf_tuned.predict_proba(X_val)[:, 1]\n\nbest_thresh_vote = 0.5\nbest_f1_vote = 0\n\nfor thresh in np.arange(0.3, 0.7, 0.01):\n    preds = (voting_probs >= thresh).astype(int)\n    score = f1_score(y_val, preds)\n    if score > best_f1_vote:\n        best_f1_vote = score\n        best_thresh_vote = thresh\n\nprint(f\"[Voting] Best Threshold: {best_thresh_vote:.2f}, Best F1 Score: {best_f1_vote:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T17:00:43.394920Z","iopub.execute_input":"2025-07-23T17:00:43.395225Z","iopub.status.idle":"2025-07-23T17:00:43.841649Z","shell.execute_reply.started":"2025-07-23T17:00:43.395197Z","shell.execute_reply":"2025-07-23T17:00:43.840838Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Best Threshold: 0.45, Best F1 Score: 0.6463\n","metadata":{}},{"cell_type":"markdown","source":"# Criteria 10\n## Comparison of model performances","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\nvalidation_f1_scores = {\n    'Logistic Regression': 0.5460,\n    'Decision Tree': 0.5102,\n    'Random Forest': 0.5921,\n    'Gradient Boosting': 0.6051,\n    'XGBoost': 0.6243,\n    'KNN': 0.5778,\n    'Neural Net (MLP)': 0.5968,\n    'AdaBoost': 0.5993,\n    'SGDClassifier': 0.5374,\n    'VotingClassifier': 0.6349,\n    \n    'XGBoost (Tuned)': 0.6305,\n    'Gradient Boosting (Tuned)': 0.6102,\n    'MLPClassifier (Tuned)': 0.6010,\n    'VotingClassifier (Tuned)': 0.6463,  \n}\n\nf1_comparison_df = pd.DataFrame.from_dict(validation_f1_scores, orient='index', columns=['F1 Score'])\nf1_comparison_df = f1_comparison_df.sort_values(by='F1 Score', ascending=False)\n\nprint(\"F1 Score Comparison Across Models (Validation Set Only):\")\nprint(f1_comparison_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T17:00:43.842632Z","iopub.execute_input":"2025-07-23T17:00:43.842931Z","iopub.status.idle":"2025-07-23T17:00:43.852449Z","shell.execute_reply.started":"2025-07-23T17:00:43.842903Z","shell.execute_reply":"2025-07-23T17:00:43.851753Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_probs = voting_clf_tuned.predict_proba(test_df)[:, 1]\ntest_preds = (test_probs >= 0.45).astype(int)  # Use best threshold\n\nsubmission_df = pd.DataFrame({\n    'id': test_df['id'],\n    'exit_status': test_preds\n})\n\nsubmission_df.to_csv('submission.csv', index=False)\nprint(\"Submission file created using threshold 0.40 and tuned ensemble.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T17:00:43.853394Z","iopub.execute_input":"2025-07-23T17:00:43.853701Z","iopub.status.idle":"2025-07-23T17:00:44.163100Z","shell.execute_reply.started":"2025-07-23T17:00:43.853673Z","shell.execute_reply":"2025-07-23T17:00:44.162276Z"}},"outputs":[],"execution_count":null}]}